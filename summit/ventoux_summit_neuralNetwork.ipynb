{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d225502-60a4-45aa-a64e-7d9b43a2b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as tian\n",
    "tian.set_option('display.max_columns', None)\n",
    "import numpy as lumpnump\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import datetime\n",
    "import gcsfs\n",
    "from google.cloud import storage as gcs\n",
    "from google.oauth2 import service_account as gsa\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import certifi\n",
    "\n",
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "top_tracks_stem = os.getenv('TOP_TRACKS_STEM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a6c43-5cf2-4508-a6ce-cca87ab99cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# System variables\n",
    "google_client_id = os.getenv('GOOGLE_CLIENT_ID')\n",
    "google_client_secret = os.getenv('GOOGLE_CLIENT_SECRET')\n",
    "google_project_id = os.getenv('GOOGLE_MUSIC_PROJECT')\n",
    "google_bucket = os.getenv('GOOGLE_PRIMARY_BUCKET')\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.getenv('GOOGLE_SERVICE_CREDENTIALS')\n",
    "\n",
    "# GCS setup details\n",
    "bucket_file_base = f\"gs://{google_bucket}/\"\n",
    "gcp_storage = gcs.Client()\n",
    "gcp_primary_bucket = gcp_storage.bucket(google_bucket)\n",
    "gcp_file_system = gcsfs.GCSFileSystem()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca211c5-c53a-47a1-ac04-3d7e630b63fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## importing bigger files and creating subset I want for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fff969-11e5-42c9-a834-fb35745ba7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toptracks_bpmeta_matrix_csv = f\"gs://{google_bucket}/silent_ascent_filestoptracks_bpmeta_matrix\"\n",
    "# toptracks_bpmeta_matrix_df = f\"gs://{google_bucket}/silent_ascent_filestoptracks_bpmeta_matrix_df\"\n",
    "# toptracks_bpmeta_matrix_df_csv = '/Users/kevinkirby/Desktop/ml_local_csvs/toptracks_bpmeta_matrix_df'\n",
    "# toptracks_bpmeta_matrix_df = tian.read_csv(toptracks_bpmeta_matrix_df_csv)\n",
    "\n",
    "# #matrix\n",
    "# try:\n",
    "#   fs = gcsfs.GCSFileSystem(project=google_project_id, token=os.getenv('GOOGLE_APPLICATION_CREDENTIALS'))\n",
    "#   with fs.open(toptracks_bpmeta_matrix_csv, 'rb') as f:\n",
    "#     toptracks_bpmeta_matrix = lumpnump.genfromtxt(f, delimiter=',')\n",
    "#     print('Download complete')\n",
    "# except Exception as e:\n",
    "#   print(f\"Error: {e}\")\n",
    "\n",
    "# #df\n",
    "# try:\n",
    "#   fs = gcsfs.GCSFileSystem(project=google_project_id, token=os.getenv('GOOGLE_APPLICATION_CREDENTIALS'))\n",
    "#   with fs.open(toptracks_bpmeta_matrix_df, 'rb') as f:\n",
    "#     toptracks_bpmeta_matrix_df = tian.read_csv(toptracks_bpmeta_matrix_df)\n",
    "#     print('Download complete')\n",
    "# except Exception as e:\n",
    "#   print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f7af30-6b73-42dd-a8c6-a6319e064e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toptracks_bpmeta_matrix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc3bc40-90b1-4e20-9e0b-43a70fe174bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toptracks_bpmeta_matrix_df_columns = toptracks_bpmeta_matrix_df.columns\n",
    "# toptracks_bpmeta_matrix_df_dict = {name: idx for idx, name in enumerate(toptracks_bpmeta_matrix_df_columns)}\n",
    "# toptracks_bpmeta_matrix_df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6520b-3c52-45a8-baca-f374df65aff8",
   "metadata": {},
   "source": [
    "### eda + smaller file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ebf743-384c-4545-ad73-9ba8efa8fc71",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### eda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18cc01d-bb63-44ae-854b-f7314c1b6263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toptracks_bpmeta_matrix_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b17e5-6107-4118-aa6d-8e67e0415cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creates some charts to help figure out where the data hangs out\n",
    "# def metric_breakouts(dataframe, metric, percent_intervals):\n",
    "#     interval = tian.cut(dataframe[metric], bins=percent_intervals)\n",
    "#     metric_rows = dataframe.groupby(interval, observed=False).size()\n",
    "#     intervals_df = metric_rows.reset_index()\n",
    "#     intervals_df.columns = [f'{metric} range', 'row_count']\n",
    "#     all_rows = intervals_df['row_count'].sum()\n",
    "#     intervals_df['% of total'] = (intervals_df['row_count'] / all_rows) * 100\n",
    "#     intervals_df = intervals_df.sort_values(by='row_count', ascending=False)\n",
    "#     return intervals_df\n",
    "\n",
    "# def count_unique_in_intervals(dataframe, interval_field, count_field, interval):\n",
    "#     dataframe = dataframe.sort_values(by=interval_field, ascending=False)\n",
    "#     num_intervals = 100 // interval\n",
    "#     intervals = tian.cut(range(len(dataframe)), bins=num_intervals, labels=[f'{i*interval}-{(i+1)*interval}%' for i in range(num_intervals)])\n",
    "#     dataframe['interval'] = intervals\n",
    "#     unique_rows = dataframe.groupby('interval')[count_field].nunique().reset_index(name='unique rows')\n",
    "    \n",
    "#     return unique_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2e610-e745-4c9f-9003-c1d13ff35ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_points = toptracks_bpmeta_matrix_df.groupby('label_id')['points'].sum()\n",
    "# total_points = label_points.sum()\n",
    "# top_20_labels = label_points.nlargest(20)\n",
    "# top_20_labels_percentage = (top_20_labels / total_points) * 100\n",
    "# top_labels_df = tian.DataFrame({\n",
    "#     'points_sum': top_20_labels,\n",
    "#     'percentage_of_total': top_20_labels_percentage\n",
    "# })\n",
    "\n",
    "# print(top_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d51a96-0dae-4c3d-b3f7-6bac1a1396fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_df = count_unique_in_intervals(toptracks_bpmeta_matrix_df, 'points', 'label_id', 10)\n",
    "# print(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a1831-16b6-4d3b-b8b0-b5b4e1267e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpm_interval = range(0, 310, 10)\n",
    "# bpm_metrics_df = metric_breakouts(toptracks_bpmeta_matrix_df, 'bpm', bpm_interval)\n",
    "# bpm_metrics_df = bpm_metrics_df.sort_values(by='bpm range', ascending=True)\n",
    "# print(bpm_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9ed7d-61d4-4ed5-8ea0-97221f4b107f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### histogram and box plots by metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f77839-ce27-4260-9b39-60e858be6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "histo_box_plot(toptracks_bpmeta_matrix_df, metric_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a731aa-e7a9-4a09-8e27-f81ee51840c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### time series histogram by metric and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7028395e-ade8-457b-8d2e-1e43d2478c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_histo(toptracks_bpmeta_matrix_df, metric_ids, 'release_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee28703a-3a24-446b-a9c2-07ccb0f972d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### smaller df/matrix based on eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dad33c-6f15-43ae-85ad-d60fc528ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #columns to have in final new df \n",
    "# selected_columns = ['isrc_numeric', 'release_year', 'label_percent_interval', 'genre_id_x', \n",
    "#                     'duration', 'bpm_group', 'key_id', 'mode', 'valence', 'points']\n",
    "\n",
    "# # clearing out years that I don't need\n",
    "# filtered_df = toptracks_bpmeta_matrix_df.loc[\n",
    "#     (toptracks_bpmeta_matrix_df['release_year'] >= 2019.0) & \n",
    "#     (toptracks_bpmeta_matrix_df['release_year'] <= 2024.0)\n",
    "# ].copy()\n",
    "\n",
    "# # grouping labels in % intervals based on points \n",
    "# filtered_df.loc[:, 'label_percent_interval'] = tian.qcut(filtered_df['points'], 20, labels=False) / 20.0\n",
    "\n",
    "# #bpm range \n",
    "# bpm_range = [0, 60, 100, 110, 120, 130, 140, 150, 300]\n",
    "# filtered_df.loc[:, 'bpm_group'] = tian.cut(filtered_df['bpm'], bpm_range, right=False)\n",
    "\n",
    "# # smashing bpm ranges together to create float \n",
    "# filtered_df['bpm_group'] = filtered_df['bpm_group'].apply(lambda x: float(f\"{int(x.left)}{int(x.right)}\"))\n",
    "\n",
    "# pogi_soar_matrix_df = filtered_df[selected_columns]\n",
    "# pogi_soar_matrix = pogi_soar_matrix_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd46918-ed4c-4634-b80a-dac56bda7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pogi_soar_matrix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b2f53-f84d-44a2-bb5b-7996e4c477e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pogi_soar_matrix_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738fe9f5-3a91-4118-8077-1748b5166e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcp export\n",
    "# pogi_soar_matrix_df_csv = \"pogi_soar_matrix_df.csv\"\n",
    "# pogi_soar_matrix_csv = \"pogi_soar_matrix.csv\"\n",
    "\n",
    "# pogi_soar_matrix_df.to_csv(pogi_soar_matrix_df_csv, index=False)\n",
    "# lumpnump.savetxt(pogi_soar_matrix_csv, pogi_soar_matrix, delimiter=',')\n",
    "\n",
    "# # Upload to Google Cloud Storage\n",
    "# def upload_to_gcs(source_file_name, destination_blob_name):\n",
    "#     blob = gcp_primary_bucket.blob(destination_blob_name)\n",
    "#     blob.upload_from_filename(source_file_name)\n",
    "\n",
    "# upload_to_gcs(pogi_soar_matrix_df_csv, f\"top_tracks_tables/{pogi_soar_matrix_df_csv}\")\n",
    "# upload_to_gcs(pogi_soar_matrix_csv, f\"top_tracks_tables/{pogi_soar_matrix_csv}\")\n",
    "\n",
    "# print(\"upload complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6123ebe-e0f0-405f-8cbd-1e2d315c66d4",
   "metadata": {},
   "source": [
    "## Import of new and smaller matrix from GCP\n",
    "This is so I don't have to run all of the above every time. I've saved it to show my work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d0f9be-aa7a-4460-ade2-914d5ed5b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pogi_soar_matrix_csv = f\"gs://{google_bucket}/silent_ascent_files/pogi_soar_matrix.csv\"\n",
    "\n",
    "try:\n",
    "  fs = gcsfs.GCSFileSystem(project=google_project_id, token=os.getenv('GOOGLE_APPLICATION_CREDENTIALS'))\n",
    "  with fs.open(pogi_soar_matrix_csv, 'rb') as f:\n",
    "    pogi_soar_matrix = lumpnump.genfromtxt(f, delimiter=',')\n",
    "    print('Download complete')\n",
    "except Exception as e:\n",
    "  print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# pogi_soar_matrix_csv_local = '/Users/kevinkirby/Desktop/ml_local_csvs/pogi_soar_matrix.csv'\n",
    "# pogi_soar_matrix = lumpnump.genfromtxt(pogi_soar_matrix_csv_local, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36168114-be8a-4989-a254-3fa23c1305e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a dictionary of indexes from the matrix and a corresponding df not imported and then some values. \n",
    "pogi_soar_dict = {\n",
    "    0: {'name': 'isrc_numeric',\n",
    "        'sample_value_df': 7218192100041,\n",
    "        'sample_value_matrix': 7218192100041,\n",
    "        'unique_count': 75701},\n",
    "    1: {'name': 'release_year',\n",
    "        'sample_value_df': 2021,\n",
    "        'sample_value_matrix': 2021,\n",
    "        'unique_count': 5},\n",
    "    2: {'name': 'label_percent_interval',\n",
    "        'sample_value_df': 0.95,\n",
    "        'sample_value_matrix': 0.95,\n",
    "        'unique_count': 20},\n",
    "    3: {'name': 'genre_id_x',\n",
    "        'sample_value_df': 12,\n",
    "        'sample_value_matrix': 12,\n",
    "        'unique_count': 32},\n",
    "    4: {'name': 'duration',\n",
    "        'sample_value_df': 445,\n",
    "        'sample_value_matrix': 445,\n",
    "        'unique_count': 693},\n",
    "    5: {'name': 'bpm_group',\n",
    "        'sample_value_df': 120130.0,\n",
    "        'sample_value_matrix': 120120.0,\n",
    "        'unique_count': 8},\n",
    "    6: {'name': 'key_id',\n",
    "        'sample_value_df': 4,\n",
    "        'sample_value_matrix': 4,\n",
    "        'unique_count': 34},\n",
    "    7: {'name': 'mode',\n",
    "        'sample_value_df': 0,\n",
    "        'sample_value_matrix': 0,\n",
    "        'unique_count': 2},\n",
    "    8: {'name': 'valence',\n",
    "        'sample_value_df': 0.473,\n",
    "        'sample_value_matrix': 0.473,\n",
    "        'unique_count': 1678},\n",
    "    9: {'name': 'points',\n",
    "        'sample_value_df': 35920,\n",
    "        'sample_value_matrix': 35920,\n",
    "        'unique_count': 7286}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b6dee-ee7c-41a4-a312-315e3338d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cat_decision_boundary(ax, X, predict, class_labels=None, legend=False, vector=True, color='g', lw=1):\n",
    "    pad = 0.5\n",
    "    x_min, x_max = X[:, 0].min() - pad, X[:, 0].max() + pad\n",
    "    y_min, y_max = X[:, 1].min() - pad, X[:, 1].max() + pad\n",
    "    h = max(x_max - x_min, y_max - y_min) / 200\n",
    "    xx, yy = lumpnump.meshgrid(lumpnump.arange(x_min, x_max, h), lumpnump.arange(y_min, y_max, h))\n",
    "    points = lumpnump.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    if vector:\n",
    "        Z = predict(points)\n",
    "    else:\n",
    "        Z = lumpnump.zeros((len(points),))\n",
    "        for i in range(len(points)):\n",
    "            Z[i] = predict(points[i].reshape(1, 2))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contour(xx, yy, Z, colors=color, linewidths=lw)\n",
    "    ax.axis('tight')\n",
    "\n",
    "def plt_mc_data(ax, X, y, classes, class_labels=None, map=plt.cm.Paired, legend=False, size=50, m='o', equal_xy=False):\n",
    "    for i in range(classes):\n",
    "        idx = lumpnump.where(y == i)\n",
    "        col = len(idx[0]) * [i]\n",
    "        label = class_labels[i] if class_labels else \"c{}\".format(i)\n",
    "        ax.scatter(X[idx, 0], X[idx, 1], marker=m, color=map(col), vmin=0, vmax=map.N, s=size, label=label)\n",
    "    if legend:\n",
    "        ax.legend()\n",
    "    if equal_xy:\n",
    "        ax.axis(\"equal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574550f-7182-4fd1-91a1-9aa8781419fc",
   "metadata": {},
   "source": [
    "### neural work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75c6eec-f7f4-452d-abe6-9d318ef383a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#release year (cat), label percent interval (cat), key id (cat), mode (cat)\n",
    "two_layer_inputs = [1, 2, 6, 7]\n",
    "inputs = pogi_soar_matrix[:, two_layer_inputs]\n",
    "#0,1,2,3 \n",
    "\n",
    "# genre ID (categorical), bpm_group (cat), valence (continuous value numeric)\n",
    "two_layer_targets = [3, 5, 8]\n",
    "targets = pogi_soar_matrix[:, two_layer_targets]\n",
    "#0,1,2\n",
    "\n",
    "year_uniques = {sub_array[0] for sub_array in inputs}\n",
    "label_percent_uniques = {sub_array[1] for sub_array in inputs}\n",
    "key_uniques = {sub_array[2] for sub_array in inputs}\n",
    "mode_uniques = {sub_array[3] for sub_array in inputs}\n",
    "\n",
    "print(f'input years: {year_uniques}')\n",
    "print(f'input label %: {label_percent_uniques}')\n",
    "print(f'input keys: {key_uniques}')\n",
    "print(f'input modes: {mode_uniques}')\n",
    "\n",
    "genre_uniques = {sub_array[0] for sub_array in targets}\n",
    "bpm_uniques = {sub_array[1] for sub_array in targets}\n",
    "valence_uniques = list({sub_array[2] for sub_array in targets})\n",
    "\n",
    "print(f'target genres: {genre_uniques}')\n",
    "print(f'target bpms: {bpm_uniques}')\n",
    "print(f'target valence: {valence_uniques[:10]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c3792-ccfc-4e3f-bf6d-542f18c466da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_transform_data(data):\n",
    "    transformer = QuantileTransformer(output_distribution='normal')\n",
    "    return transformer.fit_transform(data.reshape(-1, 1)).flatten()\n",
    "\n",
    "categorical_transformer = OneHotEncoder(sparse_output=False)\n",
    "numerical_transformer = MinMaxScaler()\n",
    "\n",
    "valence_index = two_layer_targets.index(8)\n",
    "valence_data = targets[:, valence_index]\n",
    "valence_transformed = quantile_transform_data(valence_data)\n",
    "targets[:, valence_index] = valence_transformed\n",
    "\n",
    "input_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('categorical', categorical_transformer, [0, 1, 2, 3])  \n",
    "    ]\n",
    ")\n",
    "\n",
    "target_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('categorical', categorical_transformer, [0, 1]),  \n",
    "        ('numerical', numerical_transformer, [2])        \n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Original matrix shape:\", pogi_soar_matrix.shape)\n",
    "\n",
    "# Transform inputs and outputs and check shapes\n",
    "inputs_transformed = input_preprocessor.fit_transform(inputs)\n",
    "print(\"Transformed inputs shape:\", inputs_transformed.shape)\n",
    "\n",
    "targets_transformed = target_preprocessor.fit_transform(targets)\n",
    "print(\"Transformed targets shape:\", targets_transformed.shape)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(inputs_transformed, targets_transformed, test_size=0.40, random_state=1)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=1)\n",
    "\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"X_cv.shape:\", X_cv.shape)\n",
    "print(\"y_cv.shape:\", y_cv.shape)\n",
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)\n",
    "\n",
    "# Extract each input feature from the transformed inputs\n",
    "X_train_year = X_train[:, 0].reshape(-1, 1)\n",
    "X_train_label = X_train[:, 1].reshape(-1, 1)\n",
    "X_train_key = X_train[:, 2].reshape(-1, 1)\n",
    "X_train_mode = X_train[:, 3].reshape(-1, 1)\n",
    "\n",
    "print(\"X_train_year:\", X_train_year.shape)\n",
    "print(\"X_train_label:\", X_train_label.shape)\n",
    "print(\"X_train_key:\", X_train_key.shape)\n",
    "print(\"X_train_mode:\", X_train_mode.shape)\n",
    "\n",
    "X_cv_year = X_cv[:, 0].reshape(-1, 1)\n",
    "X_cv_label = X_cv[:, 1].reshape(-1, 1)\n",
    "X_cv_key = X_cv[:, 2].reshape(-1, 1)\n",
    "X_cv_mode = X_cv[:, 3].reshape(-1, 1)\n",
    "\n",
    "print(\"X_cv_year:\", X_cv_year.shape)\n",
    "print(\"X_cv_label:\", X_cv_label.shape)\n",
    "print(\"X_cv_key:\", X_cv_key.shape)\n",
    "print(\"X_cv_mode:\", X_cv_mode.shape)\n",
    "\n",
    "X_test_year = X_test[:, 0].reshape(-1, 1)\n",
    "X_test_label = X_test[:, 1].reshape(-1, 1)\n",
    "X_test_key = X_test[:, 2].reshape(-1, 1)\n",
    "X_test_mode = X_test[:, 3].reshape(-1, 1)\n",
    "\n",
    "print(\"X_test_year:\", X_test_year.shape)\n",
    "print(\"X_test_label:\", X_test_label.shape)\n",
    "print(\"X_test_key:\", X_test_key.shape)\n",
    "print(\"X_test_mode:\", X_test_mode.shape)\n",
    "\n",
    "y_train_genre = y_train[:, 0]\n",
    "y_train_bpm = y_train[:, 1]\n",
    "y_train_valence = y_train[:, 2]\n",
    "\n",
    "print(\"y_train_genre:\", y_train_genre.shape)\n",
    "print(\"y_train_bpm:\", y_train_bpm.shape)\n",
    "print(\"y_train_valence:\", y_train_valence.shape)\n",
    "\n",
    "y_cv_genre = y_cv[:, 0]\n",
    "y_cv_bpm = y_cv[:, 1]\n",
    "y_cv_valence = y_cv[:, 2]\n",
    "\n",
    "print(\"y_cv_genre:\", y_cv_genre.shape)\n",
    "print(\"y_cv_bpm:\", y_cv_bpm.shape)\n",
    "print(\"y_cv_valence:\", y_cv_valence.shape)\n",
    "\n",
    "y_test_genre = y_test[:, 0]\n",
    "y_test_bpm = y_test[:, 1]\n",
    "y_test_valence = y_test[:, 2]\n",
    "\n",
    "print(\"y_test_genre:\", y_test_genre.shape)\n",
    "print(\"y_test_bpm:\", y_test_bpm.shape)\n",
    "print(\"y_test_valence:\", y_test_valence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6ebcf-65cf-4b60-81c9-fcfce684ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pogi_prep():\n",
    "    soigneur = Sequential([\n",
    "        Input(shape=(1,)),\n",
    "        Dense(25, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dense(17, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    ])\n",
    "    return soigneur\n",
    "\n",
    "label_input = Input(shape=(1,), name='label_percent_interval')\n",
    "key_input = Input(shape=(1,), name='key')\n",
    "mode_input = Input(shape=(1,), name='mode')\n",
    "release_year_input = Input(shape=(1,), name='release_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798afd3-c08b-458d-8791-9f1747092a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pogi_slay():\n",
    "    label_features = pogi_prep()(label_input)\n",
    "    key_features = pogi_prep()(key_input)\n",
    "    mode_features = pogi_prep()(mode_input)\n",
    "    release_year_features = pogi_prep()(release_year_input)\n",
    "\n",
    "    false_flat = Concatenate()([label_features, key_features, mode_features, release_year_features])\n",
    "\n",
    "    dolly_hidden = Dense(13, activation='relu', kernel_regularizer=l2(0.1))(false_flat)\n",
    "    pogi_hidden = Dense(10, activation='relu', kernel_regularizer=l2(0.1))(dolly_hidden)\n",
    "\n",
    "    genre_slay = Dense(32, activation='softmax', name='genre_id')(pogi_hidden)\n",
    "    bpm_4tofloor = Dense(8, activation='softmax', name='bpm_group')(pogi_hidden)\n",
    "    valence_smile = Dense(1, activation='sigmoid', name='valence')(pogi_hidden)\n",
    "    \n",
    "    pogi_wheels = Model(inputs=[label_input, key_input, mode_input, release_year_input], \n",
    "                        outputs=[genre_slay, bpm_4tofloor, valence_smile])\n",
    "\n",
    "    return pogi_wheels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a4a996-3719-4acc-a612-369c8a76ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ventoux_summit = pogi_slay()\n",
    "ventoux_summit.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        'genre_id': SparseCategoricalCrossentropy(),\n",
    "        'bpm_group': SparseCategoricalCrossentropy(),\n",
    "        'valence': MeanSquaredError()\n",
    "    },\n",
    "    metrics={\n",
    "        'genre_id': ['accuracy', Precision(), Recall()],\n",
    "        'bpm_group': ['accuracy', Precision(), Recall()],\n",
    "        # 'valence': ['mae','mean_squared_error', 'RootMeanSquaredError'],\n",
    "        'valence': 'mean_squared_error',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a178cd00-c5f4-4ccc-8e19-2af04d5be75b",
   "metadata": {},
   "source": [
    "### model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b817579-e05d-4967-8393-b752d88366c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pogi_stem = '/Users/kevinkirby/Desktop/project_silent_ascent/pogi_logs'\n",
    "pogi_logger = f'{pogi_stem}/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}-ventoux_summit'\n",
    "# pogi_stem = f'{bucket_file_base}/silent_ascent_files/pogi_logs'\n",
    "# pogi_logger = f'{pogi_stem}/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}-ventoux_summit'\n",
    "\n",
    "\n",
    "pogi_monitoring = TensorBoard(log_dir=pogi_logger, histogram_freq=1)\n",
    "\n",
    "# fitting\n",
    "pogi_soar = ventoux_summit.fit(\n",
    "    [X_train_year, X_train_label, X_train_key, X_train_mode], \n",
    "    {'genre_id': y_train_genre, 'bpm_group': y_train_bpm, 'valence': y_train_valence}, \n",
    "    epochs=10, \n",
    "    batch_size=100,\n",
    "    validation_data=(\n",
    "        [X_cv_year, X_cv_label, X_cv_key, X_cv_mode], \n",
    "        {'genre_id': y_cv_genre, 'bpm_group': y_cv_bpm, 'valence': y_cv_valence}\n",
    "    ),\n",
    "    callbacks=[pogi_monitoring]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d50883a-7469-459a-bba6-979b0c168816",
   "metadata": {},
   "source": [
    "### plot predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7059f31b-404c-40ef-a062-963871b52510",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ventoux_summit.predict([X_test_year, X_test_label, X_test_key, X_test_mode])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dedb192-1b6f-46f5-bbcd-0882401c28b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ventoux_evaluation = ventoux_summit.evaluate([X_test_year, X_test_label, X_test_key, X_test_mode], y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8afb15-b4cc-4f5f-988a-0029f2b9dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions\n",
    "predictions_genre = predictions[0]\n",
    "print(f'genre{predictions_genre}')\n",
    "predictions_bpm = predictions[1]\n",
    "print(f'bpm{predictions_bpm}')\n",
    "predictions_valence = predictions[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3344f-33d7-43c3-af40-73d4f376ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions\n",
    "predictions_genre = predictions[0]\n",
    "predictions_bpm = predictions[1]\n",
    "predictions_valence = predictions[2]\n",
    "\n",
    "# Convert probabilities to predicted classes\n",
    "predicted_genre_classes = lumpnump.argmax(predictions_genre, axis=1)\n",
    "predicted_bpm_classes = lumpnump.argmax(predictions_bpm, axis=1)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Use seaborn heatmap for better visualization\n",
    "sns.heatmap(predictions_genre, cmap='viridis', xticklabels=[f'Genre {i+1}' for i in range(predictions_genre.shape[1])], yticklabels=False)\n",
    "\n",
    "plt.xlabel('Genre Classes')\n",
    "plt.ylabel('Samples')\n",
    "plt.title('Predicted Probabilities for Each Genre Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007466bb-4a78-4b85-a853-64a7ea4ab4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_tf(history):\n",
    "    fig,ax = plt.subplots(1,1, figsize = (4,3))\n",
    "    widgvis(fig)\n",
    "    ax.plot(history.history['loss'], label='loss')\n",
    "    ax.set_ylim([0, 2])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('loss (cost)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328a68a-67f9-4822-a770-e92062d4945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # terminal command for tensorboard\n",
    "# !tensorboard --logdir=/Users/kevinkirby/Desktop/project_silent_ascent/pogi_logs\n",
    "# tensorboard --logdir=gs://love-uwsthoughts/silent_ascent_files/pogi_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1827b-b421-4f2e-a998-72dc492c7cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930ea056-6784-4990-b6c0-7e88e0fe92da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow GPU env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
